{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using_Attention_to_classify_MNIST_images_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2QcmyokWOIHo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Input, layers, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, Softmax, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import tensorflow as tf\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "Y_train = to_categorical(Y_train)\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocabs = 256\n",
        "class DotProduct(layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "  def call(self, t_queries, t_keys, t_values):\n",
        "    softmax_layer = Softmax(axis = -1)\n",
        "    softmax_output = softmax_layer(tf.matmul(t_queries, tf.transpose(t_keys, [0, 2, 1]))/np.sqrt(t_queries.shape[2]))\n",
        "    return tf.matmul(softmax_output, t_values)"
      ],
      "metadata": {
        "id": "LUqy8JEbQV6-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input((X_train.shape[1]))\n",
        "n_embedding = 128\n",
        "qk_hidden = 256\n",
        "v_hidden = 256\n",
        "embedding = Embedding(n_vocabs, n_embedding, input_length = X_train.shape[1])(input)\n",
        "t_queries = Dense(qk_hidden)(embedding)\n",
        "t_keys = Dense(qk_hidden)(embedding)\n",
        "t_values = Dense(v_hidden)(embedding)\n",
        "values_probability = DotProduct()(t_queries, t_keys, t_values)\n",
        "flatten = Flatten()(values_probability)\n",
        "dense = Dense(128, activation = \"relu\")(flatten)\n",
        "output = Dense(10, activation = \"softmax\")(dense)\n",
        "model_attention = Model(input, output)\n",
        "#adam_optimizer = Adam(learning_rate = 1e-4)\n",
        "model_attention.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])\n",
        "with tf.device(\"/gpu:0\"):\n",
        "  model_attention.fit(X_train, Y_train, batch_size = 32, epochs = 30, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyczMMG0VyUg",
        "outputId": "f899adec-a97d-4a75-aece-84b7d317aa4b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 64s 32ms/step - loss: 0.4609 - acc: 0.8472\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.1678 - acc: 0.9495\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.1433 - acc: 0.9564\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.1271 - acc: 0.9612\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.1152 - acc: 0.9659\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.1052 - acc: 0.9680\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0895 - acc: 0.9725\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0815 - acc: 0.9749\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0702 - acc: 0.9779\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0658 - acc: 0.9795\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0558 - acc: 0.9828\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0525 - acc: 0.9846\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0536 - acc: 0.9847\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0436 - acc: 0.9878\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0460 - acc: 0.9880\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0412 - acc: 0.9888\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0485 - acc: 0.9881\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0428 - acc: 0.9899\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.0407 - acc: 0.9900\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0446 - acc: 0.9903\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0374 - acc: 0.9913\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0391 - acc: 0.9911\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.0485 - acc: 0.9904\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0367 - acc: 0.9923\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0430 - acc: 0.9913\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0416 - acc: 0.9925\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.0554 - acc: 0.9913\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.0416 - acc: 0.9925\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.0432 - acc: 0.9928\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.0439 - acc: 0.9934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nếu k dùng attention layer phía trước mà dùng các pixels ảnh làm feature luôn.\n",
        "input = Input((X_train.shape[1]))\n",
        "dense1 = Dense(128, activation = \"relu\")(input)\n",
        "output = Dense(10, activation = \"softmax\")(dense1)\n",
        "model_mlp = Model(input, output)\n",
        "model_mlp.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])\n",
        "with tf.device(\"/gpu:0\"):\n",
        "  model_mlp.fit(X_train, Y_train, batch_size = 32, epochs = 30, verbose = 1)\n",
        "#Hiệu năng thấp hơn là training với Attention Layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO0Qc54CJxni",
        "outputId": "9a1e05c3-409a-4fe0-d4d0-91343e19627b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 2.2348 - acc: 0.8658\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3382 - acc: 0.9168\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2723 - acc: 0.9315\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2490 - acc: 0.9384\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2354 - acc: 0.9402\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2182 - acc: 0.9445\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2044 - acc: 0.9477\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1945 - acc: 0.9510\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1920 - acc: 0.9530\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1852 - acc: 0.9549\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1806 - acc: 0.9555\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1706 - acc: 0.9579\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1694 - acc: 0.9580\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1723 - acc: 0.9582\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1613 - acc: 0.9591\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1695 - acc: 0.9593\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1678 - acc: 0.9599\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1656 - acc: 0.9612\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1610 - acc: 0.9622\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1546 - acc: 0.9631\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1544 - acc: 0.9641\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1512 - acc: 0.9648\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1463 - acc: 0.9649\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1476 - acc: 0.9645\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1487 - acc: 0.9642\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1438 - acc: 0.9648\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1514 - acc: 0.9643\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1414 - acc: 0.9658\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1436 - acc: 0.9654\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1423 - acc: 0.9660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_test, Y_test) = mnist.load_data()[1]\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "model_attention.evaluate(X_test, Y_test)\n",
        "model_mlp.evaluate(X_test, Y_test)\n",
        "#Dùng thêm Attention Layer có hiệu năng tốt hơn trên tập test\n",
        "#Tuy nhiên thời gian training dùng thêm attention layer rất lâu ! dù đã có GPU của colab"
      ],
      "metadata": {
        "id": "VggapgHylxX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f71788a-ee1e-4c09-8c94-4b94978c6a4f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 12ms/step - loss: 0.7064 - acc: 0.9644\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4464 - acc: 0.9440\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4464186131954193, 0.9440000057220459]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}